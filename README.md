# papers
These are papers which I think are worth reading. No particular order. My interests focus on
hyperparameter/architecture optimization, semi-supervised learning, and computer vision.
NF is not found, WBR is will be released. Importance is just a vague estimate of the importance to me. 
TODO means I haven't read in sufficient detail yet. I haven't always looked very hard for all the 
associated links and I might have messed up some of them. PRs with more papers or fixs are welcome. 

| name | arXiv | pdf | github | reddit | open review | desc | misc | importance |
|---|---|---|---|---|---|---|---|---|
| AdaBound | [arxiv](https://arxiv.org/abs/1607.01097) | [pdf](https://arxiv.org/pdf/1607.01097.pdf) | [github](https://github.com/Luolc/AdaBound) | [reddit](https://old.reddit.com/r/MachineLearning/comments/auvj3q/r_adabound_an_optimizer_that_trains_as_fast_as/) | [open review](https://openreview.net/forum?id=Bkg3g2R9FX) | optimizer | new, not proven | 5/10 |
| Evaluating the Search Phase of NAS | [arxiv](https://arxiv.org/abs/1902.08142) | [pdf](https://arxiv.org/pdf/1902.08142) | WBR |[reddit](https://www.reddit.com/r/MachineLearning/comments/atwnlh/r_evaluating_the_search_phase_of_neural/) | NF | evaluating NAS approaches vs random search | TODO | 8/10 |
| Random Search and Reproducibility for NAS | [arxiv](https://arxiv.org/abs/1902.07638) | [pdf](https://arxiv.org/pdf/1902.07638) | [github](https://github.com/liamcli/randomNAS_release) |[reddit](https://www.reddit.com/r/MachineLearning/comments/atebq8/r_random_search_and_reproducibility_for_neural/) | NF | evaluating NAS approaches vs random search | TODO | 8/10 |
| Neural Architecture Optimization | [arxiv](https://arxiv.org/abs/1808.07233) | [pdf](https://arxiv.org/pdf/1808.07233.pdf) | [github](https://github.com/renqianluo/NAO) |[reddit](https://www.reddit.com/r/MachineLearning/comments/9butdc/r_neural_architecture_optimization/) | NF | Uses encoder decoder set up for gradient based NAS |  | 4/10 |
| DARTS: Differentiable Architecture Search | [arxiv](https://arxiv.org/abs/1806.09055) | [pdf](https://arxiv.org/pdf/1806.09055) | [original](https://github.com/quark0/darts) [alternative](https://github.com/khanrc/pt.darts) |[reddit](https://www.reddit.com/r/MachineLearning/comments/8tzzf0/r_darts_differentiable_architecture_search/) | [open review](https://openreview.net/forum?id=S1eYHoC5FX) | Uses scalar multiples of branches for continuous relaxation | Alternate implemention is cleaner, but less verified | 8/10 |
| Efficient Neural Architecture Search via Parameter Sharing | [arxiv](https://arxiv.org/abs/1802.03268) | [pdf](https://arxiv.org/pdf/1802.03268) | [github](https://github.com/melodyguan/enas) | [reddit](https://www.reddit.com/r/MachineLearning/comments/7wxdbw/r_efficient_neural_architecture_search_via/) | [open review](https://openreview.net/forum?id=ByQZjx-0-) | Like original NAS/PNAS, but with weight sharing | implemention is in TF | 5/10 |
| Learning Transferable Architectures for Scalable Image Recognition | [arxiv](https://arxiv.org/abs/1707.07012) | [pdf](https://arxiv.org/pdf/1707.07012.pdf) | [just the model](https://github.com/wandering007/nasnet-pytorch) | [reddit](https://www.reddit.com/r/MachineLearning/comments/6pcurc/r_learning_transferable_architectures_for/) | NF |  original NASNet | [medium](https://towardsdatascience.com/everything-you-need-to-know-about-automl-and-neural-architecture-search-8db1863682bf) | 7/10 |
| ProxylessNAS | [arxiv](https://arxiv.org/abs/1812.00332) | [pdf](https://arxiv.org/pdf/1812.00332) | WBR [just the model](https://github.com/MIT-HAN-LAB/ProxylessNAS) | [reddit](https://www.reddit.com/r/MachineLearning/comments/a3a1xy/r_proxylessnas_direct_neural_architecture_search/) | [open review](https://openreview.net/forum?id=HylVB3AqYm) | architecture search which adjusts branch probabilities and optimizes for latency | | 8/10 |
